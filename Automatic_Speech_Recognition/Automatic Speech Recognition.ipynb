{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb35bbfd",
   "metadata": {},
   "source": [
    "### Automatic Speech Recognition with Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a4d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9924b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio =  \"Sample Board Recording.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e07fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2aa3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487ac830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Good morning everyone. Let's call this meeting to order. First on our agenda is the review of last quarter's financial performance. CFO, could you please lead us through the main highlights? Thank you. I'm pleased to report that we've seen a 7% increase in revenue compared to the previous quarter. This is largely due to our successful launch of the new product line, which has been well received in the market.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(sample_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4981920b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "result = pipe(sample_audio, generate_kwargs={\"language\": \"english\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8104015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Good morning everyone. Let's call this meeting to order. First on our agenda is the review of last quarter's financial performance. CFO, could you please lead us through the main highlights? Thank you. I'm pleased to report that we've seen a 7% increase in revenue compared to the previous quarter. This is largely due to our successful launch of the new product line, which has been well received in the market.\",\n",
       " 'chunks': [{'timestamp': (0.0, 8.0),\n",
       "   'text': \" Good morning everyone. Let's call this meeting to order. First on our agenda is the review of last quarter's financial performance.\"},\n",
       "  {'timestamp': (8.0, 12.0),\n",
       "   'text': ' CFO, could you please lead us through the main highlights?'},\n",
       "  {'timestamp': (12.0, 19.0),\n",
       "   'text': \" Thank you. I'm pleased to report that we've seen a 7% increase in revenue compared to the previous quarter.\"},\n",
       "  {'timestamp': (19.0, 25.46),\n",
       "   'text': ' This is largely due to our successful launch of the new product line, which has been well received in the market.'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933309c7",
   "metadata": {},
   "source": [
    "## To Do: Add Pyannote into the details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d3d79",
   "metadata": {},
   "source": [
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization = pipeline(\"audio.wav\")m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
