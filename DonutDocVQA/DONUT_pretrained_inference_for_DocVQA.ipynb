{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936AEu38RIGk"
   },
   "source": [
    "# Using DONUT for Document Visual Question Answering (DocVQA) pretrained model\n",
    "\n",
    "\n",
    "ðŸ¤— Transformers and SentencePiece are required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjTQkhuwRF6Z",
    "outputId": "296910d5-6d0a-4b45-9cde-ee8bd048d7b0"
   },
   "source": [
    "#!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL1LKdBNR9yH"
   },
   "source": [
    "## Load libraries model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOmnuWOiRhdV",
    "outputId": "651fe6ba-37ff-42c2-b24b-b6734c2dd4cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wengz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = Path(\"./Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_file_menu = {'id'}\n",
    "for i,file in image_folder.iterdir():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ny4NN58Uq-"
   },
   "source": [
    "## Prepare using processor\n",
    "\n",
    "We prepare the image for the model using `DonutProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZ6AvLnJ8wLI",
    "outputId": "cd927542-7259-49fd-f668-6c4adb53bead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2560, 1920])\n"
     ]
    }
   ],
   "source": [
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cMUZg6C7tXa"
   },
   "source": [
    "## Generate\n",
    "\n",
    "Finally, we let the model autoregressively generate the answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dh1z80dlRz8g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n",
    "question = \"When is the coffee break?\"\n",
    "prompt = task_prompt.replace(\"{user_input}\", question)\n",
    "decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "outputs = model.generate(pixel_values.to(device),\n",
    "                               decoder_input_ids=decoder_input_ids.to(device),\n",
    "                               max_length=model.decoder.config.max_position_embeddings,\n",
    "                               early_stopping=True,\n",
    "                               pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                               eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                               use_cache=True,\n",
    "                               num_beams=1,\n",
    "                               bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTuhkOORUt50",
    "outputId": "c4ea95bf-df0a-4353-b773-38039e4ca1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_question> When is the coffee break?</s_question><s_answer> 11-14 to 11:39 a.m.</s_answer>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "seq = processor.batch_decode(outputs.sequences)[0]\n",
    "seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQamK_phkYS6"
   },
   "source": [
    "## Convert to JSON\n",
    "\n",
    "We can convert the generated sequence to JSON if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDW-_3wPEHPi",
    "outputId": "c73399a7-e437-4978-b1bf-04e6b94d047b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '11-14 to 11:39 a.m.', 'question': 'When is the coffee break?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.token2json(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X4Mc8KA9E_K3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPkCw+duSXzvWcLKQEkiZqa",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Quick inference with DONUT for DocVQA.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
